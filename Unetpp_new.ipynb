{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Unetpp_new.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1Zd0mIIdLHsMFAexdpee3po5sRZdzLESS",
      "authorship_tag": "ABX9TyM7SlkQ6OzwuKVRajRggIJN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/penguin1109/MOAI/blob/main/Unetpp_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dbpAqjGlbXv"
      },
      "source": [
        "- 매번 새롭게 trainset과 valid set을 나누어 주었었기 때문에 unbalanced class문제를 해결하기 위해서 적용해야 하는 loss의 weigth를 매번 새롭게 계산했다.\n",
        "- classification을 해 주기 위해서 마지막 출력층에 sigmoid를 사용하였다.\n",
        "- 각 pixel이 유일한 class로 각각 분류되어야 하는 상황이기 때문에 이와 같은 경우에는 softmax보다 loss값에 처벌을 더 강하게 하는 log softmax를 적용하는 것이 낫다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugeh-w1rJCpP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19819063-cfbe-45a0-e6af-85e756e4951e"
      },
      "source": [
        "!pip install pydicom\n",
        "!pip install albumentations==0.4.6"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydicom\n",
            "  Downloading pydicom-2.2.1-py3-none-any.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 7.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: pydicom\n",
            "Successfully installed pydicom-2.2.1\n",
            "Collecting albumentations==0.4.6\n",
            "  Downloading albumentations-0.4.6.tar.gz (117 kB)\n",
            "\u001b[K     |████████████████████████████████| 117 kB 7.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.4.1)\n",
            "Collecting imgaug>=0.4.0\n",
            "  Downloading imgaug-0.4.0-py2.py3-none-any.whl (948 kB)\n",
            "\u001b[K     |████████████████████████████████| 948 kB 57.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (3.13)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (4.1.2.30)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.7.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (3.2.2)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (0.16.2)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.15.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (7.1.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2.6.2)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.8.2)\n",
            "Building wheels for collected packages: albumentations\n",
            "  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for albumentations: filename=albumentations-0.4.6-py3-none-any.whl size=65172 sha256=0bfd0b267c08ada355cab2bd6328fdc530c2ab6295a0496bd408e9de2baa3913\n",
            "  Stored in directory: /root/.cache/pip/wheels/cf/34/0f/cb2a5f93561a181a4bcc84847ad6aaceea8b5a3127469616cc\n",
            "Successfully built albumentations\n",
            "Installing collected packages: imgaug, albumentations\n",
            "  Attempting uninstall: imgaug\n",
            "    Found existing installation: imgaug 0.2.9\n",
            "    Uninstalling imgaug-0.2.9:\n",
            "      Successfully uninstalled imgaug-0.2.9\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-0.4.6 imgaug-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8yC5d2E-0gi"
      },
      "source": [
        "# Hyperparameters\n",
        "import torch\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "LEARNING_RATE = 3e-4\n",
        "BATCH_SIZE = 4\n",
        "NUM_EPOCHS = 17\n",
        "IMAGE_WIDTH = 512\n",
        "IMAGE_HEIGHT = 512\n",
        "LOAD_MODEL = True\n",
        "PIN_MEMORY = True\n",
        "NUM_WORKERS = 1\n",
        "TRAIN_IMAGE_DIR = '/content/drive/Shareddrives/Kaggle_MOAI2021/data/train/DICOM'\n",
        "TRAIN_MASK_DIR = '/content/drive/Shareddrives/Kaggle_MOAI2021/data/train/Label'\n",
        "TEST_IMAGE_DIR = '/content/drive/Shareddrives/Kaggle_MOAI2021/data/test'\n",
        "\n",
        "SIGMOID = False\n",
        "CHECKPOINT_DIR = '/content/drive/Shareddrives/Kaggle_MOAI2021/checkpoints/unetppFin_checkpoint.pth.tar'\n",
        "FILENAME = 'unetppFin_checkpoint.pth.tar'\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CPyvUtIrO9F"
      },
      "source": [
        "import cv2\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pydicom import dcmread\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "def get_mask_bool(mask, type = None):\n",
        "    mask_1 = mask < 0.00784314\n",
        "    mask_2 = mask >= 0.00392157\n",
        "    mask_3 = np.logical_and(mask_1, mask_2)\n",
        "    if type == 'tumor':\n",
        "        mask = (mask >= 0.00784314).astype(float)\n",
        "    elif type == 'organ':\n",
        "        mask = mask_3.astype(float)\n",
        "    elif type == \"all\":\n",
        "        mask = (mask!=0).astype(float)\n",
        "    else:\n",
        "        mask = (mask < 0.00392157).astype(float)\n",
        "    if 1.0 not in mask:\n",
        "      return False\n",
        "    else:\n",
        "      return True\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXVb1bCnrQmw"
      },
      "source": [
        "def list_with(type, image_dir, mask_dir):\n",
        "  dcom_dirs = np.array(sorted(glob(image_dir + '/*/*')))\n",
        "  mask_dirs = np.array(sorted(glob(mask_dir + '/*/*')))\n",
        "  imgs, masks = [],[]\n",
        "  if type == 'tumor':\n",
        "    for idx, dir in enumerate(mask_dirs):\n",
        "      mask = mpimg.imread(dir)\n",
        "      mask = np.around(mask, 8)\n",
        "      if get_mask_bool(mask, 'tumor'):\n",
        "        imgs.append(dcom_dirs[idx])\n",
        "        masks.append(mask_dirs[idx])\n",
        "  elif type == 'organ':\n",
        "    for idx, dir in enumerate(mask_dirs):\n",
        "      mask = mpimg.imread(dir)\n",
        "      mask = np.around(mask, 8)\n",
        "      if get_mask_bool(mask, 'organ'):\n",
        "        imgs.append(dcom_dirs[idx])\n",
        "        masks.append(mask_dirs[idx])\n",
        "  else:\n",
        "    for idx, dir in enumerate(mask_dirs):\n",
        "      mask = mpimg.imread(dir)\n",
        "      mask = np.around(mask, 8)\n",
        "      if get_mask_bool(mask, 'all'):\n",
        "        imgs.append(dcom_dirs[idx])\n",
        "        masks.append(mask_dirs[idx])\n",
        "\n",
        "  return imgs, masks\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_stLc4MrSqL"
      },
      "source": [
        "#dcom_dirs, mask_dirs = list_with('all', TRAIN_IMAGE_DIR, TRAIN_MASK_DIR)\n",
        "#dcom_dirs, mask_dirs = np.array(dcom_dirs), np.array(mask_dirs)\n",
        "#print(len(dcom_dirs), len(mask_dirs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ToELVdEIxuF"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "import numpy as np\n",
        "import pydicom\n",
        "from sklearn.metrics import jaccard_similarity_score\n",
        "\n",
        "def transform_to_hu(medical_image, image):\n",
        "    hu_image = image * medical_image.RescaleSlope + medical_image.RescaleIntercept\n",
        "    hu_image[hu_image < -1024] = -1024\n",
        "    return hu_image\n",
        "\n",
        "def window_image(image, window_center, window_width):\n",
        "    window_image = image.copy()\n",
        "    image_min = window_center - (window_width / 2)\n",
        "    image_max = window_center + (window_width / 2)\n",
        "    window_image[window_image < image_min] = image_min\n",
        "    window_image[window_image > image_max] = image_max\n",
        "    return window_image\n",
        "\n",
        "def resize_normalize(image):\n",
        "    image = np.array(image, dtype=np.float64)\n",
        "    image -= np.min(image)\n",
        "    image /= np.max(image)\n",
        "    return image\n",
        "\n",
        "def read_dicom(path, window_widht, window_level, channel):\n",
        "    image_medical = pydicom.dcmread(path)\n",
        "    image_data = image_medical.pixel_array\n",
        "\n",
        "    image_hu = transform_to_hu(image_medical, image_data)\n",
        "    image_window = window_image(image_hu.copy(), window_level, window_widht)\n",
        "    image_window_norm = resize_normalize(image_window)\n",
        "\n",
        "\n",
        "    image_window_norm = np.expand_dims(image_window_norm, axis=-1)   # (512, 512, 1)\n",
        "    if channel == 3:\n",
        "      image_ths = np.concatenate([image_window_norm, image_window_norm, image_window_norm], axis=2)   # (512, 512, 3)\n",
        "    else:\n",
        "      image_ths = image_window_norm\n",
        "    return image_ths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2exZNmuo8pXb"
      },
      "source": [
        "import torch\n",
        "import argparse\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def str2bool(v):\n",
        "    if v.lower() in ['true', 1]:\n",
        "        return True\n",
        "    elif v.lower() in ['false', 0]:\n",
        "        return False\n",
        "    else:\n",
        "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
        "\n",
        "\n",
        "def count_params(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "def save_checkpoint(state, filename = FILENAME):\n",
        "    root = '/content/drive/Shareddrives/Kaggle_MOAI2021/checkpoints'\n",
        "    print(\"=>saving checkpoint...\")\n",
        "    torch.save(state, root + '/' + filename)\n",
        "\n",
        "def load_checkpoint(checkpoint, model):\n",
        "    print(\"=>loading checkpoint...\")\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "def get_mask(mask, type = None):\n",
        "    mask_1 = mask < 0.00784314\n",
        "    mask_2 = mask >= 0.00392157\n",
        "    mask_3 = np.logical_and(mask_1, mask_2)\n",
        "    if type == 'tumor':\n",
        "        mask = (mask >= 0.00784314).astype(float)\n",
        "    elif type == 'organ':\n",
        "        mask = mask_3.astype(float)\n",
        "    elif type == \"all\":\n",
        "        mask = (mask!=0).astype(float)\n",
        "    else:\n",
        "        mask = (mask < 0.00392157).astype(float)\n",
        "    return mask\n",
        "\n",
        "def get_loaders(train_dir,valid_dir, batch_size, train_transform, valid_transform, num_workers = 1, pin_memory = True):\n",
        "    # 매번 validataion dataset을 새롭게 설정해 준다고 하면 (전체 dataset에서 비율을 정해서 해줌)\n",
        "    train_ds = MOAIDataset(\n",
        "        train_dir,\n",
        "        transform = train_transform\n",
        "    )\n",
        "    \n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size = batch_size, shuffle = True,\n",
        "        num_workers = num_workers, pin_memory=pin_memory\n",
        "    )\n",
        "\n",
        "    valid_ds = MOAIDataset(\n",
        "        valid_dir,\n",
        "        transform = valid_transform\n",
        "    )\n",
        "\n",
        "    valid_loader = DataLoader(\n",
        "        valid_ds, \n",
        "        batch_size = batch_size, shuffle = True,\n",
        "        num_workers = num_workers, pin_memory = pin_memory\n",
        "    )\n",
        "\n",
        "    return train_loader, valid_loader\n",
        "\n",
        "def check_accuracy(loader, scheduler, model, device = \"cuda\"):\n",
        "    num_correct = 0\n",
        "    num_pixels = 0\n",
        "    dice_score = 0\n",
        "    model.eval()  # unet모델의 가중치를 학습하는게 아니라 evaluation 단계에 들어갔음을 의미하는 명령어\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for image, mask in loader:\n",
        "            image = image.to(device)\n",
        "            mask = mask.permute(0, 3, 1, 2).to(device)\n",
        "            print('making predictions...')\n",
        "            if SIGMOID:\n",
        "              predictions = model(image.float())\n",
        "            else:\n",
        "              predictions = torch.sigmoid(model(image.float()))\n",
        "            print(\"prediction made...\")\n",
        "\n",
        "            result = (predictions > 0.5)[0].float().cpu().permute(1, 2, 0).detach().numpy()\n",
        "            visualize_mask(to_cpu(predictions)[0].permute(1, 2, 0).numpy())\n",
        "            visualize_mask(result)\n",
        "\n",
        "            predictions = (predictions > 0.5).float()\n",
        "            num_correct += (predictions == mask).sum()\n",
        "            num_pixels += torch.numel(predictions)\n",
        "            dice_score += (2 * (predictions * mask).sum()) / ((predictions + mask).sum() + 1e-9)\n",
        "            \n",
        "    scheduler.step(dice_score/len(loader))\n",
        "        \n",
        "    print(f\"Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.3f}\")\n",
        "    print(f\"Got DICE Score : {dice_score/len(loader)}\")\n",
        "    model.train()\n",
        "\n",
        "def visualize(inputs, outputs, masks, channel):\n",
        "  if not SIGMOID:\n",
        "    outputs = torch.sigmoid(outputs)\n",
        "    \n",
        "  if torch.is_tensor(inputs):\n",
        "    inputs = inputs.data.detach().cpu()\n",
        "\n",
        "  if torch.is_tensor(outputs):\n",
        "    outputs = (outputs).data.detach().cpu()\n",
        "    outputs = (outputs>0.5).float()\n",
        "    \n",
        "  if torch.is_tensor(masks):\n",
        "    masks = masks.data.detach().cpu()\n",
        "\n",
        "  if channel == 1:\n",
        "    for i in range(BATCH_SIZE):\n",
        "      imgs = [inputs[i,:,:,0], outputs[i,0,:,:], masks[i,:,:,0]]\n",
        "      plt.figure(figsize = (6, 6))\n",
        "      for j in range(len(imgs)):\n",
        "        plt.subplot(1, len(imgs), j+1)\n",
        "        plt.imshow(imgs[j])\n",
        "      plt.show()\n",
        "  elif channel == 2:\n",
        "    for i in range(BATCH_SIZE):\n",
        "      imgs = [inputs[i,0,:,:], outputs[i].permute(1, 2, 0)[:,:,0],masks[i,:,:,0],outputs[i].permute(1, 2, 0)[:,:,1],masks[i,:,:,1]]\n",
        "      plt.figure(figsize = (10, 10))\n",
        "      for j in range(len(imgs)):\n",
        "        plt.subplot(1, len(imgs), j+1)\n",
        "        plt.imshow(imgs[j])\n",
        "      plt.show()\n",
        "  \n",
        "  else:\n",
        "    res = make_pred(outputs.permute(0,2,3,1))\n",
        "    for i in range(BATCH_SIZE):\n",
        "      imgs = [inputs[i].permute(1, 2, 0)[:,:,0], outputs[i].permute(1, 2, 0)[:,:,0],outputs[i].permute(1, 2, 0)[:,:,1],outputs[i].permute(1, 2, 0)[:,:,2],res[i,:,:,0], masks[i]]\n",
        "      plt.figure(figsize = (10, 10))\n",
        "      for j in range(len(imgs)):\n",
        "        plt.subplot(1, len(imgs), j+1)\n",
        "        plt.imshow(imgs[j])\n",
        "      plt.show()\n",
        "\n",
        "def make_pred(output):\n",
        "  x = np.zeros((output.shape[0], output.shape[1],output.shape[2], 1))\n",
        "\n",
        "  m_all = (output[:,:,:,0] == 0)\n",
        "  m_or = (output[:,:,:,1] == 1)\n",
        "  m_tu = (output[:,:,:,2] == 1)\n",
        "  m_organ = np.logical_and(m_all, m_or)\n",
        "  m_tu_2 = np.logical_and(m_all, (m_or == 0))\n",
        "\n",
        "  #x[:,:,:,0][m_all == 1] = 1\n",
        "  #x[:,:,:,0][m_or == 1] = 1\n",
        "  x[:,:,:,0][m_organ == 1] = 1\n",
        "  x[:,:,:,0][m_tu == 1] = 2\n",
        "  x[:,:,:,0][m_tu_2 == 1] = 2\n",
        "\n",
        "  return x\n",
        "\n",
        "def iou_score(output, target):\n",
        "    smooth = 1e-6\n",
        "\n",
        "    if torch.is_tensor(output):\n",
        "        output = output.data.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
        "    \n",
        "    if torch.is_tensor(target):\n",
        "        target = target.data.detach().cpu().numpy()\n",
        "\n",
        "    output_ = (output>0.5)\n",
        "    target_ = (target>0.5)\n",
        "\n",
        "    intersection = (output_ & target_).sum()\n",
        "    union = (output_ | target_).sum()\n",
        "\n",
        "    return (intersection + smooth) / (union + smooth)\n",
        "\n",
        "def iouScore(output, target):\n",
        "  if torch.is_tensor(output):\n",
        "    output = output.data.detach().cpu().permute(0, 2, 3, 1)\n",
        "    if not SIGMOID:\n",
        "      output = torch.sigmoid(output)\n",
        "    output = make_pred(output>0.5)\n",
        "    \n",
        "  if torch.is_tensor(target):\n",
        "    target = target.data.detach().cpu().numpy()\n",
        "  \n",
        "  target = np.argmax(target, axis = -1)\n",
        "\n",
        "  jac = 0.0\n",
        "  for i in range(4):\n",
        "    out, tar = output[i], target[i]\n",
        "    out = np.reshape(out, 512*512)\n",
        "    tar = np.reshape(tar, 512*512)\n",
        "    jac += jaccard_similarity_score(out, tar, normalize = True)\n",
        "\n",
        "  return jac / 4.0\n",
        "\n",
        "  return jac\n",
        "\n",
        "def dice_coef(output, target):\n",
        "    smooth = 1e-6\n",
        "\n",
        "    output = output.view(-1).data.cpu().numpy()\n",
        "    target = target.view(-1).data.cpu().numpy()\n",
        "    intersection = (output * target).sum()\n",
        "\n",
        "    return (2. * intersection + smooth) / (output.sum() + target.sum() + smooth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdLJ9Iu58ARb"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BCEDiceLoss(nn.Module):\n",
        "    def __init__(self, t_normedWeights, v_normedWeights):\n",
        "        super().__init__()\n",
        "        self.t_weight = t_normedWeights\n",
        "        self.v_weight = v_normedWeights\n",
        "\n",
        "    def forward(self, inputs, targets, mode):\n",
        "        if mode == 'train':\n",
        "          if not SIGMOID:\n",
        "            bce = F.binary_cross_entropy_with_logits(inputs, targets, weight = self.t_weight, reduction = 'mean')\n",
        "            smooth = 1e-6\n",
        "\n",
        "            inputs = torch.sigmoid(inputs)\n",
        "          else:\n",
        "            bce = F.binary_cross_entropy(inputs, targets, weight = self.t_weight, reduction = 'mean')\n",
        "            smooth = 1e-6\n",
        "\n",
        "          dims = (1, 2)\n",
        "          for i in range(3):\n",
        "            input, target = inputs[:, i, :, :], targets[:, i, :, :]\n",
        "            intersection = torch.sum(input * target, dims) * self.t_weight[i]\n",
        "            cardinality = torch.sum(input + target, dims) * self.t_weight[i]\n",
        "\n",
        "        else:\n",
        "          if not SIGMOID:\n",
        "            bce = F.binary_cross_entropy_with_logits(inputs, targets, weight = self.v_weight, reduction = 'mean')\n",
        "            smooth = 1e-6\n",
        "\n",
        "            inputs = torch.sigmoid(inputs)\n",
        "          else:\n",
        "            bce = F.binary_cross_entropy(inputs, targets, weight = self.v_weight, reduction = 'mean')\n",
        "            smooth = 1e-6\n",
        "\n",
        "          dims = (1, 2)\n",
        "          for i in range(3):\n",
        "            input, target = inputs[:, i, :, :], targets[:, i, :, :]\n",
        "            intersection = torch.sum(input * target, dims) * self.v_weight[i]\n",
        "            cardinality = torch.sum(input + target, dims) * self.v_weight[i]\n",
        "\n",
        "        \n",
        "\n",
        "        dice_score = 2. * (smooth+intersection) / (cardinality + smooth)\n",
        "        dice_score = torch.mean(1. - dice_score)\n",
        "\n",
        "        return 0.5 * (bce + dice_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZDVZtYS8qTt"
      },
      "source": [
        "import cv2\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pydicom import dcmread\n",
        "import matplotlib.image as mpimg\n",
        "#from utils import get_mask\n",
        "\n",
        "class MOAIDataset(Dataset):\n",
        "    def __init__(self, dir, transform = None):\n",
        "        self.mask_dir = dir\n",
        "        self.file_no = list(map(lambda x: x.split('/')[-2] +'/'+ (x.split('/')[-1].split('.')[0] + '.dcm'),self.mask_dir))\n",
        "        self.image_dir = list(map(lambda x: TRAIN_IMAGE_DIR + '/'+ x, self.file_no))\n",
        "        self.transform = transform\n",
        "\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.image_dir)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        img_path = self.image_dir[index]  # .dcm으로 저장된 DCM format의 CT 데이터\n",
        "        mask_path = self.mask_dir[index]  # .png로 저장된 PNG format의 ground truth data\n",
        "        \n",
        "\n",
        "        image = read_dicom(img_path, 100, 50, 1)\n",
        "        mask = mpimg.imread(mask_path)\n",
        "        mask = np.around(mask, 8)\n",
        "\n",
        "        m_bg = get_mask(mask)\n",
        "        m_tu = get_mask(mask, 'tumor')\n",
        "        m_or = get_mask(mask, 'organ')\n",
        "\n",
        "        m_bg = np.expand_dims(m_bg, axis = -1)\n",
        "        m_tu = np.expand_dims(m_tu, axis = -1)\n",
        "        m_or = np.expand_dims(m_or, axis = -1)\n",
        "        m_fn = np.concatenate([m_bg, m_or, m_tu], axis = -1)\n",
        "        #m_fn = np.concatenate([m_or, m_tu], axis = -1)\n",
        "        #m_fn = np.zeros((512, 512, 1))\n",
        "        #m_fn[m_tu == 1] = 2\n",
        "        #m_fn[m_or == 1] = 1\n",
        "\n",
        "        \n",
        "        # mask = np.array(cv2.imread(mask_path)[:,:,0]) # shape = (512, 512)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            augmentations = self.transform(image = image, mask = m_fn)\n",
        "            image = augmentations['image']\n",
        "            m_fn = augmentations['mask']\n",
        "\n",
        "        return image, m_fn.float()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Co0qqwj880i2"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "class VGGBlock(nn.Module):\n",
        "    def __init__(self, ch_in, ch_mid, ch_out):\n",
        "        super().__init__()\n",
        "        self.relu = nn.ReLU(inplace = True)\n",
        "        self.conv1 = nn.Conv2d(ch_in, ch_mid, kernel_size = 3, padding = 1)\n",
        "        self.bn1 = nn.BatchNorm2d(ch_mid)\n",
        "        self.conv2 = nn.Conv2d(ch_mid, ch_out, kernel_size = 3, padding = 1)\n",
        "        self.bn2 = nn.BatchNorm2d(ch_out)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class UNetPP(nn.Module):\n",
        "    def __init__(self,ch_in,ch_out, deep_supervision = True, sigmoid = SIGMOID):\n",
        "        super().__init__()\n",
        "\n",
        "        #nb_filter = [16, 32, 64, 128, 256]\n",
        "        nb_filter = [32, 64, 128, 256, 512]\n",
        "\n",
        "        self.deep_supervision = deep_supervision\n",
        "        self.sigmoid = sigmoid\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.up = nn.Upsample(scale_factor = 2, mode = 'bilinear', align_corners = True)\n",
        "\n",
        "        self.conv0_0 = VGGBlock(ch_in, nb_filter[0], nb_filter[0])\n",
        "        self.conv1_0 = VGGBlock(nb_filter[0], nb_filter[1], nb_filter[1])\n",
        "        self.conv2_0 = VGGBlock(nb_filter[1], nb_filter[2], nb_filter[2])\n",
        "        self.conv3_0 = VGGBlock(nb_filter[2], nb_filter[3], nb_filter[3])\n",
        "        self.conv4_0 = VGGBlock(nb_filter[3], nb_filter[4], nb_filter[4])\n",
        "\n",
        "        self.conv0_1 = VGGBlock(nb_filter[0]+nb_filter[1], nb_filter[0], nb_filter[0])\n",
        "        self.conv1_1 = VGGBlock(nb_filter[1]+nb_filter[2], nb_filter[1], nb_filter[1])\n",
        "        self.conv2_1 = VGGBlock(nb_filter[2]+nb_filter[3], nb_filter[2], nb_filter[2])\n",
        "        self.conv3_1 = VGGBlock(nb_filter[3]+nb_filter[4], nb_filter[3], nb_filter[3])\n",
        "\n",
        "        self.conv0_2 = VGGBlock(nb_filter[0]*2+nb_filter[1], nb_filter[0], nb_filter[0])\n",
        "        self.conv1_2 = VGGBlock(nb_filter[1]*2+nb_filter[2], nb_filter[1], nb_filter[1])\n",
        "        self.conv2_2 = VGGBlock(nb_filter[2]*2+nb_filter[3], nb_filter[2], nb_filter[2])\n",
        "\n",
        "        self.conv0_3 = VGGBlock(nb_filter[0]*3+nb_filter[1], nb_filter[0], nb_filter[0])\n",
        "        self.conv1_3 = VGGBlock(nb_filter[1]*3+nb_filter[2], nb_filter[1], nb_filter[1])\n",
        "\n",
        "        self.conv0_4 = VGGBlock(nb_filter[0]*4+nb_filter[1], nb_filter[0], nb_filter[0])\n",
        "\n",
        "        if self.deep_supervision:\n",
        "          if self.sigmoid:\n",
        "            self.final1 = nn.Sequential(nn.Conv2d(nb_filter[0], ch_out, kernel_size=1), nn.Sigmoid())\n",
        "            self.final2 = nn.Sequential(nn.Conv2d(nb_filter[0], ch_out, kernel_size=1), nn.Sigmoid())\n",
        "            self.final3 = nn.Sequential(nn.Conv2d(nb_filter[0], ch_out, kernel_size=1), nn.Sigmoid())\n",
        "            self.final4 = nn.Sequential(nn.Conv2d(nb_filter[0], ch_out, kernel_size=1), nn.Sigmoid())\n",
        "          else:\n",
        "            self.final1 = nn.Conv2d(nb_filter[0], ch_out, kernel_size=1)\n",
        "            self.final2 = nn.Conv2d(nb_filter[0], ch_out, kernel_size=1)\n",
        "            self.final3 = nn.Conv2d(nb_filter[0], ch_out, kernel_size=1)\n",
        "            self.final4 = nn.Conv2d(nb_filter[0], ch_out, kernel_size=1)\n",
        "\n",
        "        else:\n",
        "            self.final = nn.Conv2d(nb_filter[0], ch_out, kernel_size=1)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        x0_0 = self.conv0_0(input)\n",
        "        x1_0 = self.conv1_0(self.pool(x0_0))\n",
        "        x0_1 = self.conv0_1(torch.cat([x0_0, self.up(x1_0)], 1))\n",
        "\n",
        "        x2_0 = self.conv2_0(self.pool(x1_0))\n",
        "        x1_1 = self.conv1_1(torch.cat([x1_0, self.up(x2_0)], 1))\n",
        "        x0_2 = self.conv0_2(torch.cat([x0_0, x0_1, self.up(x1_1)], 1))\n",
        "\n",
        "        x3_0 = self.conv3_0(self.pool(x2_0))\n",
        "        x2_1 = self.conv2_1(torch.cat([x2_0, self.up(x3_0)], 1))\n",
        "        x1_2 = self.conv1_2(torch.cat([x1_0, x1_1, self.up(x2_1)], 1))\n",
        "        x0_3 = self.conv0_3(torch.cat([x0_0, x0_1, x0_2, self.up(x1_2)], 1))\n",
        "\n",
        "        x4_0 = self.conv4_0(self.pool(x3_0))\n",
        "        x3_1 = self.conv3_1(torch.cat([x3_0, self.up(x4_0)], 1))\n",
        "        x2_2 = self.conv2_2(torch.cat([x2_0, x2_1, self.up(x3_1)], 1))\n",
        "        x1_3 = self.conv1_3(torch.cat([x1_0, x1_1, x1_2, self.up(x2_2)], 1))\n",
        "        x0_4 = self.conv0_4(torch.cat([x0_0, x0_1, x0_2, x0_3, self.up(x1_3)], 1))\n",
        "\n",
        "        if self.deep_supervision:\n",
        "            output1 = self.final1(x0_1)\n",
        "            output2 = self.final2(x0_2)\n",
        "            output3 = self.final3(x0_3)\n",
        "            output4 = self.final4(x0_4)\n",
        "            return [output1, output2, output3, output4]\n",
        "\n",
        "        else:\n",
        "            output = self.final(x0_4)\n",
        "            return output\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2ZWblMOZ5CB"
      },
      "source": [
        "- DiceLosswith Logits는 점점 감소하여야 하고, IOUScore의 경우에는 계속해서 증가하여야 한다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQgJvEc9iAnC"
      },
      "source": [
        "def calc_weights(mask_dir):\n",
        "  nSamples = [0,0,0]\n",
        "  for idx, dir in enumerate(mask_dir):\n",
        "    mask = mpimg.imread(dir)\n",
        "    mask = np.round(mask, 8)\n",
        "    if get_mask_bool(mask, 'tumor'):\n",
        "      nSamples[2] += 1\n",
        "    if get_mask_bool(mask, 'organ'):\n",
        "      nSamples[1] += 1\n",
        "    nSamples[0] += 1\n",
        "  normedWeights = [1-(x/sum(nSamples)) for x in nSamples]\n",
        "  normedWeights = torch.FloatTensor(normedWeights).to(DEVICE)\n",
        "\n",
        "  return normedWeights\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXtrTqOy83uV"
      },
      "source": [
        "import os\n",
        "from collections import OrderedDict\n",
        "from glob import glob\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from albumentations.augmentations import transforms\n",
        "from albumentations.core.composition import Compose, OneOf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.optim import lr_scheduler\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train(train_loader, model, criterion, optimizer, deep_supervision = True):\n",
        "    avg_meters = {'loss': AverageMeter(),\n",
        "                  'iou': AverageMeter()}\n",
        "\n",
        "    model.train()\n",
        "    loop = tqdm(train_loader)\n",
        "\n",
        "    for idx, (image, target) in enumerate(loop):\n",
        "        image = image.to(DEVICE)\n",
        "        target = target.to(DEVICE)\n",
        "\n",
        "        # compute output\n",
        "        if deep_supervision:\n",
        "            outputs = model(image)\n",
        "            loss = 0\n",
        "            for output in outputs:\n",
        "                loss += criterion(output.permute(0,2, 3, 1).float(), target, mode = 'train')\n",
        "            loss /= len(outputs)   # BATCHSIZE\n",
        "            iou = iouScore(outputs[-1], target)\n",
        "            if idx % 50 ==0:\n",
        "              visualize(image, outputs[-1], target, 3)\n",
        "        else:\n",
        "            output = model(image)\n",
        "            loss = criterion(output, target)\n",
        "            iou = iou_score(output, target)\n",
        "\n",
        "        # compute gradient and do optimizing step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        avg_meters['loss'].update(loss.item(), image.shape[0])\n",
        "        avg_meters['iou'].update(iou, image.shape[0])\n",
        "\n",
        "        postfix = OrderedDict([\n",
        "            ('loss', avg_meters['loss'].avg),\n",
        "            ('iou', avg_meters['iou'].avg),\n",
        "        ])\n",
        "        loop.set_postfix(postfix)\n",
        "        #pbar.set_postfix(postfix)\n",
        "        #pbar.update(1)\n",
        "    #pbar.close()\n",
        "\n",
        "    return OrderedDict([('loss', avg_meters['loss'].avg),\n",
        "                        ('iou', avg_meters['iou'].avg)])\n",
        "    \n",
        "def validate(valid_loader, model, criterion, scheduler, deep_supervision = True):\n",
        "    avg_meters = {'loss': AverageMeter(),\n",
        "                  'iou': AverageMeter()}\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "    loop = tqdm(valid_loader)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for iter, (image, target) in enumerate(loop):\n",
        "            image = image.to(DEVICE)\n",
        "            target = target.to(DEVICE)\n",
        "\n",
        "            # compute output\n",
        "            if deep_supervision:\n",
        "                outputs = model(image)\n",
        "                \n",
        "                loss = 0\n",
        "                out_fin = outputs[-1]\n",
        "                for output in outputs:\n",
        "                  loss += criterion(output.permute(0, 2, 3, 1).float(), target, mode = 'valid')\n",
        "                loss /= len(outputs)\n",
        "                iou = iouScore(outputs[-1], target)\n",
        "                if iter %50 ==0:\n",
        "                    visualize(image, out_fin, target, 3)\n",
        "            else:\n",
        "                output = model(image)\n",
        "                loss = dice_coef(output.permute(0,2,3,1).float(), target)\n",
        "                iou = iou_score(output, target)\n",
        "\n",
        "            avg_meters['loss'].update(loss.item(), image.shape[0])\n",
        "            avg_meters['iou'].update(iou, image.shape[0])\n",
        "\n",
        "            postfix = OrderedDict([\n",
        "                ('loss', avg_meters['loss'].avg),\n",
        "                ('iou', avg_meters['iou'].avg),\n",
        "            ])\n",
        "            loop.set_postfix(postfix)\n",
        "            #pbar.set_postfix(postfix)\n",
        "            #pbar.update(1)\n",
        "        #pbar.close()\n",
        "\n",
        "    return OrderedDict([('loss', avg_meters['loss'].avg),\n",
        "                        ('iou', avg_meters['iou'].avg)])\n",
        "\n",
        "def main():\n",
        "    model = UNetPP(ch_in = 1, ch_out = 3, sigmoid = SIGMOID).to(DEVICE)\n",
        "    \n",
        "    optimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
        "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, patience = 2)\n",
        "\n",
        "    \n",
        "\n",
        "    \n",
        "    if LOAD_MODEL:\n",
        "      load_checkpoint(torch.load(CHECKPOINT_DIR), model)\n",
        "\n",
        "    \n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "\n",
        "      dcom_dirs = np.array(sorted(glob(TRAIN_IMAGE_DIR + '/*/*')))\n",
        "      mask_dirs = np.array(sorted(glob(TRAIN_MASK_DIR + '/*/*')))\n",
        "\n",
        "      mask_train_dirs, mask_valid_dirs = train_test_split(mask_dirs, test_size = 0.2, shuffle = True)\n",
        "\n",
        "      t_normedWeights, v_normedWeights = calc_weights(mask_train_dirs), calc_weights(mask_valid_dirs)\n",
        "\n",
        "     \n",
        "      #criterion = nn.BCELoss(weight=normedWeights).to(DEVICE)\n",
        "      criterion = BCEDiceLoss(t_normedWeights, v_normedWeights).to(DEVICE)\n",
        "\n",
        "  \n",
        "\n",
        "      train_transform = Compose([\n",
        "        transforms.RandomRotate90(always_apply = True),\n",
        "        transforms.Flip(),\n",
        "        transforms.Resize(512, 512),\n",
        "        transforms.Normalize(mean = 0.0, std = 1.0, max_pixel_value = 1.0),\n",
        "        ToTensorV2()])\n",
        "    \n",
        "      val_transform = Compose([\n",
        "        transforms.Resize(512, 512),\n",
        "        transforms.Normalize(mean = 0.0, std = 1.0, max_pixel_value = 1.0),\n",
        "        ToTensorV2()\n",
        "        ])\n",
        "\n",
        "      train_loader, valid_loader = get_loaders(\n",
        "        mask_train_dirs, mask_valid_dirs, BATCH_SIZE,\n",
        "        train_transform, val_transform, num_workers = NUM_WORKERS, pin_memory = PIN_MEMORY\n",
        "        )\n",
        "\n",
        "\n",
        "      best_iou, trigger = 0, 0\n",
        "\n",
        "      print(\"=>Training for epoch %d\" %epoch)\n",
        "\n",
        "      train_log = train(train_loader, model, criterion, optimizer)\n",
        "      valid_log = validate(valid_loader, model, criterion, scheduler)\n",
        "\n",
        "      scheduler.step(valid_log['loss'])\n",
        "\n",
        "      trigger += 1\n",
        "\n",
        "      if valid_log['iou'] > best_iou:\n",
        "          checkpoint = {\n",
        "              \"state_dict\" : model.state_dict(),\n",
        "              \"optimizer\" : optimizer.state_dict()\n",
        "                }\n",
        "          save_checkpoint(checkpoint)\n",
        "          best_iou = valid_log['iou']\n",
        "          print(\"=> SAVED BEST IOU\")\n",
        "          trigger = 0\n",
        "        \n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qJSQoBPGxjq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03704aaa-d584-4257-9d4b-0502226930df"
      },
      "source": [
        "model = UNetPP(ch_in = 1, ch_out = 3, sigmoid = SIGMOID).to(DEVICE)\n",
        "load_checkpoint(torch.load(CHECKPOINT_DIR), model)\n",
        "\n",
        "test_dir = '/content/drive/Shareddrives/Kaggle_MOAI2021/data/test/DICOM'\n",
        "test_images = np.array(sorted(glob(test_dir + '/*/*')))\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "def rle_encode(mask_image):\n",
        "    pixels = mask_image.flatten()\n",
        "    # We avoid issues with '1' at the start or end (at the corners of\n",
        "    # the original image) by setting those pixels to '0' explicitly.\n",
        "    # We do not expect these to be non-zero for an accurate mask,\n",
        "    # so this should not harm the score.\n",
        "    pixels[0] = 0\n",
        "    pixels[-1] = 0\n",
        "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 2\n",
        "    runs[1::2] = runs[1::2] - runs[:-1:2]\n",
        "    return runs\n",
        "\n",
        "\n",
        "def rle_to_string(runs):\n",
        "    return ' '.join(str(x) for x in runs)\n",
        "\n",
        "def submit_encode(mask):\n",
        "  x = np.zeros((mask.shape[0], mask.shape[1], 1))\n",
        "  \n",
        "  mask = (mask > 0.5).float()\n",
        "  # 전체 organ + tumor모든 부분을 예측\n",
        "  m_all = (mask[:,:,0] == 0.0)\n",
        "  m_tu = mask[:,:,2]\n",
        "  m_or = mask[:,:,1]\n",
        "  m_organ = np.logical_and(m_all, m_or)\n",
        "  m_tu_2 = np.logical_and(m_all, (m_or == 0.0))\n",
        "  \n",
        "  #x[:,:,0][m_all == 1] = 1\n",
        "  x[:,:,0][m_organ == 1] = 1\n",
        "  \n",
        "  # 모델의 organ예측과 all 부분 예측의 공통 부분을 organ으로 생각\n",
        "\n",
        "  x[:,:,0][m_tu == 1] = 2\n",
        "  x[:,:,0][m_tu_2 == 1] = 2\n",
        "  \n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=>loading checkpoint...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvXRCHvENs5Y"
      },
      "source": [
        "# submission을 위한 prediction값을 저장해 준다.\n",
        "# 출력값에 대해서 sigmoid 를 이용해서 0과 1사이의 값으로 바꾸어 주고\n",
        "# my_encode()함수를 사용해서 \n",
        "# channel별로 0, 1, 2의 값으로 저장해 준다.\n",
        "import torch\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "test_dir = '/content/drive/Shareddrives/Kaggle_MOAI2021/data/test/DICOM'\n",
        "test_ids = os.listdir(test_dir)\n",
        "test_images = np.array(sorted(glob(test_dir + '/*/*')))\n",
        "\n",
        "preds_test = []\n",
        "test_transform = A.Compose([\n",
        "    A.Resize(512, 512),\n",
        "    A.Normalize(\n",
        "      mean = 0.0,\n",
        "      std = 1.0,\n",
        "      max_pixel_value = 1.0\n",
        "      ),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "for n, path in tqdm(enumerate(test_images), total=len(test_images)):\n",
        "    id = path.split('/')[-2]\n",
        "    img = read_dicom(path, 100, 50,1)\n",
        "    inp = test_transform(image = img)['image']\n",
        "    inp = torch.from_numpy(np.expand_dims(inp, axis = 0)).to(DEVICE)\n",
        "\n",
        "\n",
        "    out_first = model(inp)[-1]\n",
        "    if not SIGMOID:\n",
        "      out_first = torch.sigmoid(out_first)\n",
        "    out = out_first[0].permute(1, 2, 0).float()\n",
        "    #print(np.unique(out[:,:,0]),np.unique(out[:,:,1]),np.unique(out[:,:,2]))\n",
        "    #out = (out > 0.5).float()\n",
        "    \n",
        "\n",
        "\n",
        "    out_res = submit_encode(out.detach().cpu())\n",
        "    preds_test.append(out_res)\n",
        "    plt.figure(figsize = (6,6))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(img[:,:,0])\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(out.detach().cpu())\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(out_res[:,:,0]/2)\n",
        "    plt.show()\n",
        "    print(np.unique(out_res), (out_res == 2).sum())\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxIA-DP0N3C_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56c34715-c465-4f3e-9eee-cf2d177180da"
      },
      "source": [
        "import pandas as pd\n",
        "# rlencoding방법을 적용해서 csv파일로 만들어 제출이 가능하도록 한다..\n",
        "preds_string = []\n",
        "for i in tqdm(range(0, len(preds_test), 64)):\n",
        "    sample = preds_test[i:i+64].copy()\n",
        "    for label_code in [1,2]:\n",
        "        tmp=[]\n",
        "        for s in sample:\n",
        "            s = np.equal(s, label_code).flatten()*1\n",
        "            tmp+=s.tolist()\n",
        "        enc = rle_to_string(rle_encode(np.array(tmp)))\n",
        "\n",
        "        preds_string.append(enc)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 83/83 [04:21<00:00,  3.15s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1et7BabN5rs"
      },
      "source": [
        "sample_submission = pd.read_csv('/content/drive/Shareddrives/Kaggle_MOAI2021/data/sample_submission.csv')\n",
        "sample_submission['EncodedPixels'] = preds_string\n",
        "sample_submission.to_csv('/content/drive/Shareddrives/Kaggle_MOAI2021/checkpoints/submission.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}